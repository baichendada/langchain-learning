import asyncio
import json
import logging
import os
import sys
from typing import Any, Dict, List

from dotenv import load_dotenv
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.chat_models import init_chat_model
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_mcp_adapters.client import MultiServerMCPClient

load_dotenv(override=True)
print(os.getenv("LLM_API_KEY"))
print(os.getenv("MODEL"))

class Configuration:
    """è¯»å– .env ä¸ servers_config.json"""
    def __init__(self) -> None:
        
        self.api_key: str = os.getenv("LLM_API_KEY") or ""
        self.model: str = os.getenv("MODEL") or "deepseek-chat"
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®")

    @staticmethod
    def load_servers(file_path: str = "servers_config.json") -> Dict[str, Any]:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f).get("mcpServers", {})

async def get_mcp_tools(mcp_client):
    """æ‰‹åŠ¨è·å–MCPå·¥å…·ï¼Œé¿å…ä½¿ç”¨æœ‰é—®é¢˜çš„load_mcp_toolså‡½æ•°"""
    try:
        # å°è¯•ä½¿ç”¨get_toolsæ–¹æ³•
        if hasattr(mcp_client, 'get_tools'):
            tools = await mcp_client.get_tools()
            return tools
        
        # å¦‚æœæ²¡æœ‰get_toolsï¼Œå°è¯•å…¶ä»–æ–¹æ³•
        elif hasattr(mcp_client, 'list_tools'):
            tools = await mcp_client.list_tools()
            return tools
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºåˆ—è¡¨
        else:
            print("âš ï¸ MCPå®¢æˆ·ç«¯æ²¡æœ‰å¯ç”¨çš„å·¥å…·è·å–æ–¹æ³•")
            return []
            
    except Exception as e:
        print(f"âš ï¸ è·å–MCPå·¥å…·æ—¶å‡ºé”™: {e}")
        return []
    
async def run_chat_loop() -> None:
    """ä¸»å¾ªç¯"""
    cfg = Configuration()
    servers_cfg = cfg.load_servers()

    os.environ["OPENAI_API_KEY"] = cfg.api_key

    try:
        mcp_client = MultiServerMCPClient(servers_cfg)
        
        # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„å·¥å…·è·å–å‡½æ•°
        tools = await get_mcp_tools(mcp_client)
        
        if tools:
            logging.info(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·ï¼š {[getattr(t, 'name', str(t)) for t in tools]}")
        else:
            logging.warning("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°MCPå·¥å…·ï¼Œå°†ä½¿ç”¨åŸºç¡€èŠå¤©åŠŸèƒ½")
            tools = []  # ç©ºå·¥å…·åˆ—è¡¨ï¼Œä»ç„¶å¯ä»¥è¿›è¡ŒåŸºç¡€å¯¹è¯

    except Exception as e:
        print(f"âš ï¸ MCPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ å°†ä½¿ç”¨åŸºç¡€èŠå¤©åŠŸèƒ½ï¼ˆæ— MCPå·¥å…·ï¼‰")
        tools = []
        mcp_client = None

    model = ChatTongyi(
        api_key=cfg.api_key,
        model=cfg.model,
    )

    # æ ¹æ®æ˜¯å¦æœ‰å·¥å…·å†³å®šæ˜¯å¦åˆ›å»ºagent
    if tools:
        prompt = hub.pull("hwchase17/openai-tools-agent")
        
        agent = create_openai_tools_agent(
            llm=model,
            tools=tools,
            prompt=prompt,
        )

        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True
        )
        
        print("\nğŸ¤– MCP Agent å·²å¯åŠ¨ï¼ˆå¸¦å·¥å…·ï¼‰ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    else:
        # æ²¡æœ‰å·¥å…·æ—¶ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹
        agent_executor = None
        print("\nğŸ¤– åŸºç¡€èŠå¤©æ¨¡å¼å·²å¯åŠ¨ï¼ˆæ— MCPå·¥å…·ï¼‰ï¼Œè¾“å…¥ 'quit' é€€å‡º")

    while True:
        user_input = input("\nä½ : ").strip()
        if user_input.lower() == "quit":
            break
        try:
            print("\nAI: ", end="", flush=True)  # å¼€å§‹AIå›å¤ï¼Œä¸æ¢è¡Œ
            
            if agent_executor:
                # ä½¿ç”¨Agentæ‰§è¡Œï¼ˆæœ‰å·¥å…·ï¼‰
                full_response = ""
                async for chunk in agent_executor.astream({"input": user_input}):
                    # å¤„ç†ä¸åŒç±»å‹çš„chunk
                    if isinstance(chunk, dict):
                        # æŸ¥æ‰¾è¾“å‡ºå†…å®¹
                        if "output" in chunk:
                            content = chunk["output"]
                            if content != full_response:  # é¿å…é‡å¤è¾“å‡º
                                new_content = content[len(full_response):]
                                print(new_content, end="", flush=True)
                                full_response = content
                        elif "agent" in chunk and "messages" in chunk["agent"]:
                            # å¤„ç†agentçš„ä¸­é—´æ¶ˆæ¯
                            for message in chunk["agent"]["messages"]:
                                if hasattr(message, "content") and message.content:
                                    content = message.content
                                    if content != full_response:
                                        new_content = content[len(full_response):]
                                        print(new_content, end="", flush=True)
                                        full_response = content
                        elif "steps" in chunk:
                            # å¤„ç†æ­¥éª¤ä¿¡æ¯ï¼ˆå·¥å…·è°ƒç”¨ç­‰ï¼‰
                            for step in chunk["steps"]:
                                if hasattr(step, "observation") and step.observation:
                                    # å¯ä»¥é€‰æ‹©æ˜¯å¦æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœ
                                    pass
            else:
                # ç›´æ¥ä½¿ç”¨æ¨¡å‹ï¼ˆæ— å·¥å…·ï¼‰
                from langchain_core.messages import HumanMessage
                response = await model.agenerate([[HumanMessage(content=user_input)]])
                content = response.generations[0][0].text
                print(content, end="", flush=True)
            
            print()  # æ¢è¡Œç»“æŸå½“å‰å›å¤
            
        except Exception as exc:
            print(f"\nâš ï¸  å‡ºé”™: {exc}")

    # æ¸…ç†èµ„æº
    if mcp_client:
        try:
            await mcp_client.cleanup()
            print("ğŸ§¹ MCPèµ„æºå·²æ¸…ç†")
        except:
            pass
    print("Bye!")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    asyncio.run(run_chat_loop())
