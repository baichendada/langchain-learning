import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_community.embeddings import DashScopeEmbeddings
import os
from dotenv import load_dotenv

load_dotenv(override=True)

dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

@st.cache_resource
def load_embeddings():
    """ç¼“å­˜åµŒå…¥æ¨¡å‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–"""
    return DashScopeEmbeddings(
        model="text-embedding-v1",
        dashscope_api_key=dashscope_api_key
    )

embeddings = load_embeddings()


def pdf_read(pdf_doc):
    text = ""
    for pdf in pdf_doc:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text


def get_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)
    return chunks


def vector_store(text_chunks):
    vector_store = FAISS.from_texts(text_chunks, embeddings)
    vector_store.save_local("faiss_db")


@st.cache_resource
def load_llm():
    """ç¼“å­˜å¤§è¯­è¨€æ¨¡å‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–"""
    return ChatTongyi(
        api_key=dashscope_api_key,
    )

def get_conversational_chain(tools, ques):
    model = load_llm()
    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """ä½ æ˜¯AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç¡®ä¿æä¾›æ‰€æœ‰ç»†èŠ‚ï¼Œå¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´"ç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­"ï¼Œä¸è¦æä¾›é”™è¯¯çš„ç­”æ¡ˆ""",
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])

    mytools = [tools]

    agent = create_tool_calling_agent(llm=model, tools=mytools, prompt=prompt)
    agent_executor = AgentExecutor(agent=agent, tools=mytools, verbose=True)
    response = agent_executor.invoke({
        "input": ques,
    })
    print(response)
    st.write("ğŸ¤– å›ç­”: ", response['output'])


def check_database_exists():
    return os.path.exists("faiss_db") and os.path.exists("faiss_db/index.faiss")


@st.cache_resource
def load_vector_database():
    """ç¼“å­˜å‘é‡æ•°æ®åº“ï¼Œé¿å…é‡å¤åŠ è½½"""
    if not check_database_exists():
        return None
    return FAISS.load_local("faiss_db", embeddings, allow_dangerous_deserialization=True)

def user_input(user_question):
    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if not check_database_exists():
        st.error("âŒ è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶å¹¶ç‚¹å‡»'Submit & Process'æŒ‰é’®æ¥å¤„ç†æ–‡æ¡£ï¼")
        st.info("ğŸ’¡ æ­¥éª¤ï¼š1ï¸âƒ£ ä¸Šä¼ PDF â†’ 2ï¸âƒ£ ç‚¹å‡»å¤„ç† â†’ 3ï¸âƒ£ å¼€å§‹æé—®")
        return

    try:
        db = load_vector_database()
        if db is None:
            st.error("âŒ æ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
            return

        retriever = db.as_retriever()
        retriever_tool = create_retriever_tool(retriever, "pdf_extractor",
                                               "This tool is to give answer to queries from the pdf")
        get_conversational_chain(retriever_tool, user_question)

    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        st.info("è¯·é‡æ–°å¤„ç†PDFæ–‡ä»¶")


def main():
    st.set_page_config("ğŸ¤– LangChain")
    st.header("ğŸ¤– LangChain")

    # æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€
    col1, col2 = st.columns([3, 1])

    with col1:
        if check_database_exists():
            pass
        else:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶")
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ•°æ®åº“"):
            try:
                import shutil

                if os.path.exists("faiss_db"):
                    shutil.rmtree("faiss_db")
                
                # æ¸…é™¤ç¼“å­˜
                load_vector_database.clear()
                st.success("æ•°æ®åº“å·²æ¸…é™¤")
                st.rerun()
            except Exception as e:
                st.error(f"æ¸…é™¤å¤±è´¥: {e}")

    # ç”¨æˆ·é—®é¢˜è¾“å…¥
    user_question = st.text_input("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜",
                                  placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                                  disabled=not check_database_exists())
    if user_question:
        if check_database_exists():
            with st.spinner("ğŸ¤” AIæ­£åœ¨åˆ†ææ–‡æ¡£..."):
                user_input(user_question)
        else:
            st.error("âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶ï¼")

        # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("ğŸ“ æ–‡æ¡£ç®¡ç†")

        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        if check_database_exists():
            st.success("âœ… æ•°æ®åº“çŠ¶æ€ï¼šå·²å°±ç»ª")
        else:
            st.info("ğŸ“ çŠ¶æ€ï¼šç­‰å¾…ä¸Šä¼ PDF")

        st.markdown("---")

        # æ–‡ä»¶ä¸Šä¼ 
        pdf_doc = st.file_uploader(
            "ğŸ“ ä¸Šä¼ PDFæ–‡ä»¶",
            accept_multiple_files=True,
            type=['pdf'],
            help="æ”¯æŒä¸Šä¼ å¤šä¸ªPDFæ–‡ä»¶"
        )

        if pdf_doc:
            st.info(f"ğŸ“„ å·²é€‰æ‹© {len(pdf_doc)} ä¸ªæ–‡ä»¶")
            for i, pdf in enumerate(pdf_doc, 1):
                st.write(f"{i}. {pdf.name}")

        # å¤„ç†æŒ‰é’®
        process_button = st.button(
            "ğŸš€ æäº¤å¹¶å¤„ç†",
            disabled=not pdf_doc,
            use_container_width=True
        )

        if process_button:
            if pdf_doc:
                with st.spinner("ğŸ“Š æ­£åœ¨å¤„ç†PDFæ–‡ä»¶..."):
                    try:
                        raw_text = pdf_read(pdf_doc)

                        if not raw_text.strip():
                            st.error("âŒ æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ")
                            return

                        text_chunks = get_chunks(raw_text)
                        st.info(f"ğŸ“ æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(text_chunks)} ä¸ªç‰‡æ®µ")

                        vector_store(text_chunks)
                        
                        # æ¸…é™¤å‘é‡æ•°æ®åº“ç¼“å­˜ï¼Œç¡®ä¿æ–°æ•°æ®åº“è¢«åŠ è½½
                        load_vector_database.clear()
                        
                        st.success("âœ… PDFå¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†")
                        st.balloons()
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ å¤„ç†PDFæ—¶å‡ºé”™: {str(e)}")
                    else:
                        st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©PDFæ–‡ä»¶")

        with st.expander("ğŸ’¡ ä½¿ç”¨è¯´æ˜"):
            st.markdown("""
            **æ­¥éª¤ï¼š**
            1. ğŸ“ ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶
            2. ğŸš€ ç‚¹å‡»"Submit & Process"å¤„ç†æ–‡æ¡£
            3. ğŸ’¬ åœ¨ä¸»é¡µé¢è¾“å…¥æ‚¨çš„é—®é¢˜
            4. ğŸ¤– AIå°†åŸºäºPDFå†…å®¹å›ç­”é—®é¢˜
    
            **æç¤ºï¼š**
            - æ”¯æŒå¤šä¸ªPDFæ–‡ä»¶åŒæ—¶ä¸Šä¼ 
            - å¤„ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
            - å¯ä»¥éšæ—¶æ¸…é™¤æ•°æ®åº“é‡æ–°å¼€å§‹
            """)

if __name__ == "__main__":
    main()