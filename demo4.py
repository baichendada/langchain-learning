import asyncio
import os

from dotenv import load_dotenv
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

load_dotenv(override=True)

prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(
            content="ä½ æ˜¯ä¹äºåŠ©äººçš„åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”"
        ),
        MessagesPlaceholder(variable_name="messages")
    ]
)

model = ChatTongyi(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    model = "qwen-turbo"
)

chain = prompt | model | StrOutputParser()

messages_list = []

# åˆ›å»ºå¼‚æ­¥å‡½æ•°å¤„ç†æµå¼å“åº”
async def get_streaming_response(chain, messages_list):
    response = ""
    print("ğŸ¤– çŒ«å¨˜ï¼š", end="", flush=True)
    async for chunk in chain.astream({"messages": messages_list}):
        response += chunk
        print(chunk, end="", flush=True)
    print()  # æ¢è¡Œ
    return response

while True:
    user_input = input("ğŸ‘¤ ä½ ï¼š")
    if user_input in ["quit", "q", "exit"]:
        break

    messages_list.append(HumanMessage(content=user_input))
    # response = chain.invoke({
    #     "messages":messages_list
    # })
    # print(f"ğŸ¤– çŒ«å¨˜ï¼š{response}")

    # æµå¼å›ç­”
    response = asyncio.run(get_streaming_response(chain, messages_list))

    messages_list.append(AIMessage(content=response))
    messages_list = messages_list[-50:]


# # æ¨¡æ‹Ÿå¯¹è¯
# messages_list = [
#     HumanMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯ç™½æ™¨ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"),
#     AIMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯çŒ«å¨˜ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"),
# ]
#
# question = "ä½ ç°åœ¨æ„Ÿè§‰æ€ä¹ˆæ ·ï¼Ÿ"
#
# messages_list.append(HumanMessage(content=question))
#
# print(messages_list)

# response = chain.invoke({
#     "messages":messages_list
# })
# print(response)